{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Scikit-learn hyperparameter search wrapper\n\nIaroslav Shcherbatyi, Tim Head and Gilles Louppe. June 2017.\nReformatted by Holger Nahrstaedt 2020\n\n.. currentmodule:: skopt\n\n## Introduction\n\nThis example assumes basic familiarity with\n[scikit-learn](http://scikit-learn.org/stable/index.html).\n\nSearch for parameters of machine learning models that result in best\ncross-validation performance is necessary in almost all practical\ncases to get a model with best generalization estimate. A standard\napproach in scikit-learn is using :obj:`sklearn.model_selection.GridSearchCV` class, which takes\na set of values for every parameter to try, and simply enumerates all\ncombinations of parameter values. The complexity of such search grows\nexponentially with the addition of new parameters. A more scalable\napproach is using :obj:`sklearn.model_selection.RandomizedSearchCV`, which however does not take\nadvantage of the structure of a search space.\n\nScikit-optimize provides a drop-in replacement for :obj:`sklearn.model_selection.GridSearchCV`,\nwhich utilizes Bayesian Optimization where a predictive model referred\nto as \"surrogate\" is used to model the search space and utilized to\narrive at good parameter values combination as soon as possible.\n\nNote: for a manual hyperparameter optimization example, see\n\"Hyperparameter Optimization\" notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\nimport numpy as np\n\nnp.random.seed(123)\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\nfrom skopt import BayesSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Minimal example\n\nA minimal example of optimizing hyperparameters of SVC (Support Vector machine Classifier) is given below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = load_digits(n_class=10, return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size=0.75, test_size=0.25, random_state=0\n)\n\n# log-uniform: understand as search over p = exp(x) by varying x\nopt = BayesSearchCV(\n    SVC(),\n    {\n        'C': (1e-6, 1e6, 'log-uniform'),\n        'gamma': (1e-6, 1e1, 'log-uniform'),\n        'degree': (1, 8),  # integer valued parameter\n        'kernel': ['linear', 'poly', 'rbf'],  # categorical parameter\n    },\n    n_iter=32,\n    cv=3,\n)\n\nopt.fit(X_train, y_train)\n\nprint(\"val. score: %s\" % opt.best_score_)\nprint(\"test score: %s\" % opt.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced example\n\nIn practice, one wants to enumerate over multiple predictive model classes,\nwith different search spaces and number of evaluations per class. An\nexample of such search over parameters of Linear SVM, Kernel SVM, and\ndecision trees is given below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC, LinearSVC\n\nfrom skopt import BayesSearchCV\nfrom skopt.plots import plot_histogram, plot_objective\nfrom skopt.space import Categorical, Integer, Real\n\nX, y = load_digits(n_class=10, return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n# pipeline class is used as estimator to enable\n# search over different model types\npipe = Pipeline([('model', SVC())])\n\n# single categorical value of 'model' parameter is\n# sets the model class\n# We will get ConvergenceWarnings because the problem is not well-conditioned.\n# But that's fine, this is just an example.\nlinsvc_search = {\n    'model': [LinearSVC(max_iter=1000)],\n    'model__C': (1e-6, 1e6, 'log-uniform'),\n}\n\n# explicit dimension classes can be specified like this\nsvc_search = {\n    'model': Categorical([SVC()]),\n    'model__C': Real(1e-6, 1e6, prior='log-uniform'),\n    'model__gamma': Real(1e-6, 1e1, prior='log-uniform'),\n    'model__degree': Integer(1, 8),\n    'model__kernel': Categorical(['linear', 'poly', 'rbf']),\n}\n\nopt = BayesSearchCV(\n    pipe,\n    # (parameter space, # of evaluations)\n    [(svc_search, 40), (linsvc_search, 16)],\n    cv=3,\n)\n\nopt.fit(X_train, y_train)\n\nprint(\"val. score: %s\" % opt.best_score_)\nprint(\"test score: %s\" % opt.score(X_test, y_test))\nprint(\"best params: %s\" % str(opt.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Partial Dependence plot of the objective function for SVC\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_ = plot_objective(\n    opt.optimizer_results_[0],\n    dimensions=[\"C\", \"degree\", \"gamma\", \"kernel\"],\n    n_minimum_search=int(1e8),\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot of the histogram for LinearSVC\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_ = plot_histogram(opt.optimizer_results_[1], 1)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Progress monitoring and control using `callback` argument of `fit` method\n\nIt is possible to monitor the progress of :class:`BayesSearchCV` with an event\nhandler that is called on every step of subspace exploration. For single job\nmode, this is called on every evaluation of model configuration, and for\nparallel mode, this is called when n_jobs model configurations are evaluated\nin parallel.\n\nAdditionally, exploration can be stopped if the callback returns `True`.\nThis can be used to stop the exploration early, for instance when the\naccuracy that you get is sufficiently high.\n\nAn example usage is shown below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\nfrom sklearn.svm import SVC\n\nfrom skopt import BayesSearchCV\n\nX, y = load_iris(return_X_y=True)\n\nsearchcv = BayesSearchCV(\n    SVC(gamma='scale'),\n    search_spaces={'C': (0.01, 100.0, 'log-uniform')},\n    n_iter=10,\n    cv=3,\n)\n\n\n# callback handler\ndef on_step(optim_result):\n    score = -optim_result['fun']\n    print(\"best score: %s\" % score)\n    if score >= 0.98:\n        print('Interrupting!')\n        return True\n\n\nsearchcv.fit(X, y, callback=on_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Counting total iterations that will be used to explore all subspaces\n\nSubspaces in previous examples can further increase in complexity if you add\nnew model subspaces or dimensions for feature extraction pipelines. For\nmonitoring of progress, you would like to know the total number of\niterations it will take to explore all subspaces. This can be\ncalculated with `total_iterations` property, as in the code below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\nfrom sklearn.svm import SVC\n\nfrom skopt import BayesSearchCV\n\nX, y = load_iris(return_X_y=True)\n\nsearchcv = BayesSearchCV(\n    SVC(),\n    search_spaces=[\n        ({'C': (0.1, 1.0)}, 19),  # 19 iterations for this subspace\n        {'gamma': (0.1, 1.0)},\n    ],\n    n_iter=10,\n)\n\nprint(searchcv.total_iterations)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}